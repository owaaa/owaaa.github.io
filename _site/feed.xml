<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Andrew Aslinger</title>
		<description>Big Data | Software | Architecture</description>
		<link>http://owaaa.github.io/</link>
		<atom:link href="http://owaaa.github.io/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Publishing MongoDB Stats to CloudWatch in Amazon AWS</title>
				<description>&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Amazon &lt;a href=&quot;http://aws.amazon.com/cloudwatch/&quot;&gt;CloudWatch&lt;/a&gt; is a useful tool for monitoring your compute resources, services, and applications running in AWS. Amazon automatically publishes metrics for you when you use many different AWS Services.&lt;/p&gt;

&lt;p&gt;In this Article I will show how to publish custom metrics to CloudWatch for monitoring MongoDB. These metrics can be used to drive policy for &lt;a href=&quot;http://aws.amazon.com/documentation/autoscaling/&quot;&gt;AutoScale Groups&lt;/a&gt; to automatically scale up and down your compute resources based on your custom metrics.&lt;/p&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;p&gt;In this example I am assuming use of the &lt;a href=&quot;http://aws.amazon.com/amazon-linux-ami/&quot;&gt;Amazon Linux AMI&lt;/a&gt;. Steps should be similar on any Linux AMI.&lt;/p&gt;

&lt;p&gt;For the scripting language to compute the metrics, we will use Python. Python works well for several reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It’s easy to call AWS command line tools&lt;/li&gt;

&lt;li&gt;It’s easy to script with minimal OS dependencies&lt;/li&gt;

&lt;li&gt;Libraries/Drivers exist to calling services like MongoDB&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;iam_role&quot;&gt;IAM Role&lt;/h3&gt;

&lt;p&gt;You need to grant your EC2 system rights to publish data to CloudWatch. An easy way to do this is via an IAM Role.&lt;/p&gt;

&lt;p&gt;Below is an example of a Policy that allows your EC2 instance to publish metrics to CloudWatch&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
	&amp;quot;Version&amp;quot;: &amp;quot;2012-10-17&amp;quot;,
	&amp;quot;Statement&amp;quot;: [
	 {
	   &amp;quot;Sid&amp;quot;: &amp;quot;Stmt1389897768000&amp;quot;,
	   &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;,
	   &amp;quot;Action&amp;quot;: [
   			 &amp;quot;cloudwatch:GetMetricStatistics&amp;quot;,
   			 &amp;quot;cloudwatch:ListMetrics&amp;quot;,
 		     &amp;quot;cloudwatch:PutMetricData&amp;quot;,
   			 &amp;quot;ec2:DescribeTags&amp;quot;
 	    ],
  		&amp;quot;Resource&amp;quot;:  &amp;quot;*&amp;quot;
  
    }
    ]
}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Simply either modify your machines existing IAM Role or create a new role and add this policy. The IAM Role must be associated with the ec2 instance at the time of creation.&lt;/p&gt;

&lt;h3 id=&quot;ec2_install_prerequisites&quot;&gt;EC2 Install Pre-requisites&lt;/h3&gt;

&lt;p&gt;Python is already installed on the Amazon Linux AMI. All we need is the &lt;a href=&quot;http://api.mongodb.org/python/current/&quot;&gt;PyMongo&lt;/a&gt; driver. The easiest way to get this without other dependencies is to use easy_install&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;easy_install pymongo&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can be added to a bootstrap script in your UserData section of &lt;a href=&quot;http://aws.amazon.com/cloudformation/&quot;&gt;CloudFormation&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;write_the_metric_script&quot;&gt;Write the Metric Script&lt;/h3&gt;

&lt;p&gt;Now we just need to write a simple script that will perform the mongo query, extract the metric, and then publish the metric to CloudWatch.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Import the required libraries for the script to run
import commands
import json
import pymongo
from pymongo import MongoClient

#Get the instanceId for our machine. This is important later for
#autoscaling. The dimensions we select here when publishing
#must be matched later by our autoscale policy
ret, instanceId = commands.getstatusoutput(&amp;quot;wget -q -O - http://169.254.169.254/latest/meta-data/instance-id&amp;quot;)

#Connect to MongoDB
#Here we run this script where Mongo lives so we connect to localhost
client = MongoClient(&amp;#39;localhost&amp;#39;, 27017)

#Choose the database where your data lives, here a test DB
db = client.test
#Select the collection. Here we select the jobs collection
collection = db.jobs

#Perform the query you want as the basis of the metric
#Massage the data all you want in Python before reporting
count = collection.find({&amp;quot;status&amp;quot;: &amp;quot;New&amp;quot;}).count()

#Publish the Metric to CloudWatch	
cmd = &amp;quot;aws cloudwatch put-metric-data --metric-name JobCount --namespace ExampleOrg --dimensions \&amp;quot;instance=&amp;quot; + instanceId + &amp;quot;,servertype=MongoDB\&amp;quot; --value &amp;quot; +  str(count) + &amp;quot; --region us-east-1&amp;quot;

ret,cmdout= commands.getstatusoutput(cmd)

##Cleanup
client.close()&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Park this somewhere on your image where it won’t be deleted. For example you can drop it in a folder like /opt/metrics/myMetric.py&lt;/p&gt;

&lt;p&gt;Consult the &lt;a href=&quot;http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/cloudwatch_concepts.html&quot;&gt;CloudWatch Documentation&lt;/a&gt; for the different options you can provide for your metric. You can specify additional paramters such as the Unit of Measure, timestamp, and dimensions. Note that dimensions are very important later for autoscale policies as the dimensions of your metric and the policy must match.&lt;/p&gt;

&lt;h2 id=&quot;collecting_the_metric&quot;&gt;Collecting the Metric&lt;/h2&gt;

&lt;p&gt;Once we have our mertric, we simply need the system to call it. We can accomplish this easily via a cron job.&lt;/p&gt;

&lt;p&gt;For example, to do this create a file for your job in /etc/cron.d such as /etc/cron.d/mongoMetric&lt;/p&gt;

&lt;p&gt;Inside include the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	* * * * *	root	/usr/bin/python /opt/metrics/myMetric.py&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;viewing_the_metric&quot;&gt;Viewing the Metric&lt;/h2&gt;

&lt;p&gt;Your metric will show up in the Amazon CloudWatch service console under custom metrics organized by your namespace. The dimensions you specified in the put-metric-data script will also be available as part of the metric table.&lt;/p&gt;

&lt;h2 id=&quot;whats_next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;From CloudFormation or the CloudWatch Console, you can create an Alarm where the action is either a notification or an Autoscaling Policy action. The policies for autoscaling are separate from the Cloud Watch alarms so you can have multiple metrics or alarms all call the same policy.&lt;/p&gt;</description>
				<pubDate>Tue, 14 Jan 2014 18:00:04 +0000</pubDate>
				<link>http://owaaa.github.io/cloudwatch/amazon/mongodb/devops/2014/01/14/streaming-engine-survey.html</link>
				<guid isPermaLink="true">http://owaaa.github.io/cloudwatch/amazon/mongodb/devops/2014/01/14/streaming-engine-survey.html</guid>
			</item>
		
			<item>
				<title>A Survey of Big Data Streaming Engines</title>
				<description>&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;This article is intended to provide a brief survey of Big Data “Streaming” Technologies that are currently on the market for the purpose of informing and assisting customers in selecting the best streaming product for your problem. As this is a fairly large rapidly evolving space and what encompasses a “Streaming” product is somewhat subjective, the article does not intend to be all inclusive of every product available on the market. However, it should contain most of the major players within the Open Source space.&lt;/p&gt;

&lt;p&gt;We also offer advice on factors you should consider when selecting a Streaming technology for your problem or solution. The ratings we include are &lt;a href=&quot;http://openwhere.com&quot;&gt;OpenWhere&lt;/a&gt;’s opinion based on applicability to our customer’s common problems and missions around “general” high throughput low latency streaming problems. We hope you find this information informative but as always, do your due diligence and consider how each product rates for your specific requirements.&lt;/p&gt;

&lt;h2 id=&quot;overview_and_background&quot;&gt;Overview and Background&lt;/h2&gt;

&lt;p&gt;The rapid growth of high velocity data streams such as Twitter and other social media sites, high traffic web pages, or live streaming events, has spawned the need for and development of high performance distributed technologies for rapidly ingesting and processing data in near real time. These “Streaming” big data technologies are often contrasted to traditional big data technologies such as Apache Hadoop, which can be described as operating in a “batch” or “offline” manner.&lt;/p&gt;

&lt;p&gt;Technologies such as &lt;a href=&quot;http://hadoop.apache.org/&quot;&gt;Hadoop&lt;/a&gt; are very good at answering large questions considering an entire data set at once. This is accomplished by partioning the problem (during the Map phase) and then later combining for a result via a reduction (Reduce phase of Map Reduce). These technologies are designed for scaling a process’s data storage horizontally across many servers and also try to collocate processing with the data to reduce network traffic. An example of a suitable problem for Hadoop would be computing a distribution by age or other factors for patients with a specific medical condition across a dataset of millions of medical records.&lt;/p&gt;

&lt;p&gt;While batch solutions have very high throughput compared to traditional systems enabling them to solve these large and complex problems, these systems also often exhibit very high latency. For some problems, there are requirements that necessitate the computation of a result with very little latency (milliseconds or seconds) for which traditional batch big data solutions are not suitable. These problems also require high throughput beyond the performance of a single server and often the processing only needs to be done on a single record at a time or small batches of records. These requirements drive the need for “Streaming” big data technologies. This space is still relatively immature and in active development. Many new players have been introduced into the market even within just the past few months as of the time of writing. It should also be noted that while many products can be considered to fit within this category, many of the new entrants are targeted at a specific problem within this subdomain such as &lt;a href=&quot;http://en.wikipedia.org/wiki/Complex_event_processing&quot;&gt;Complex Event Processing&lt;/a&gt; (CEP). This assessment by OpenWhere represents our opinion on the most suitable technologies for solving “general” streaming problems that our customers often face within the GIS space by considering the options available at the time of writing. The requirements for solutions often include record ingest, transformation, processing, data analytics, and loading into a persistent store. Another requirement considered is the application’s suitability for running in the cloud. Primarily Open Source solutions were considered for this study due to customer demand. &lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;streaming_requirement_considerations&quot;&gt;Streaming Requirement Considerations&lt;/h2&gt;

&lt;p&gt;When selecting a Streaming technology, there are many system factors or features enabled by the various products that you might need to consider and should be aware of when choosing a product suitable for your organization. These factors can be important based on your project requirements and it is often difficult to refactor later to another technology once one is adopted. As many of the products in this survey are Beta, they may later support these features in a future release. However, some factors are fundamental to the project’s architecture and design choices. Some important factors to consider are listed below:&lt;/p&gt;

&lt;p&gt;Consider what specific requirements your have does the product provide the following?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;At Least Once and/or Exactly Once Semantics for message processing
&lt;ul&gt;
&lt;li&gt;Exactly once is necessary for accurate counting
&lt;ul&gt;
&lt;li&gt;Either is necessary for guaranteeing a message will be processed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;

&lt;li&gt;Fault Tolerance Guarantees and Approach&lt;/li&gt;

&lt;li&gt;Transactional data streams&lt;/li&gt;

&lt;li&gt;Committing and replaying of messages&lt;/li&gt;

&lt;li&gt;Message Batching&lt;/li&gt;

&lt;li&gt;Message Windowing
&lt;ul&gt;
&lt;li&gt;(Built In / Possible via custom implementation)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;

&lt;li&gt;A high level Stream programming abstraction
&lt;ul&gt;
&lt;li&gt;(Streams / Flows / Tuples / Events)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;

&lt;li&gt;Workflows or Frameworks for managing complex Streams or Event graphs&lt;/li&gt;

&lt;li&gt;State Management (high performance and collocated with processing)
&lt;ul&gt;
&lt;li&gt;(Internal / External)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;

&lt;li&gt;Stream Aggregation, Partitioning, Routing &amp;amp; Grouping&lt;/li&gt;

&lt;li&gt;Message Ordering Guarantees&lt;/li&gt;

&lt;li&gt;Community Support, Examples, Libraries and Code Reuse&lt;/li&gt;

&lt;li&gt;Analytics support and packages available&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;streaming_performance__management_considerations&quot;&gt;Streaming Performance &amp;amp; Management Considerations&lt;/h2&gt;

&lt;p&gt;Not every Streaming product has the same performance characteristics for different data sets and processing needs due to differences in their architectures, approaches and philosophies. It should also be noted that it can be complex to deploy, monitor, and manage these complex distributed systems. Because many of these products are also immature, many don’t have the tooling support of traditional systems you might be accustomed to more mature products or from COTS vendors.&lt;/p&gt;

&lt;p&gt;Consider the following performance factors when selecting a product:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Message Throughput and horizontal scalability approach
&lt;ul&gt;
&lt;li&gt;Does the product support or require partioning?&lt;/li&gt;

&lt;li&gt;Does the product enable multiple simultaneous streams?&lt;/li&gt;

&lt;li&gt;Processing Latency factors&lt;/li&gt;

&lt;li&gt;Does the system operate in memory (like Storm) or use persistence during message exchange (Kinesis, Samza, etc.)&lt;/li&gt;

&lt;li&gt;Horizontal and Stream Scaling Strategy&lt;/li&gt;

&lt;li&gt;Partitioning &amp;amp; Message Distribution Support&lt;/li&gt;

&lt;li&gt;Inter-node communication &amp;amp; backpressure&lt;/li&gt;

&lt;li&gt;Monitoring &amp;amp; Metrics Tools and Support
&lt;ul&gt;
&lt;li&gt;Built In / Possible / None&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;

&lt;li&gt;System Processing Isolation
&lt;ul&gt;
&lt;li&gt;(None, JVM, Process, YARN, other)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;

&lt;li&gt;Stability / Replication / Fault Tolerance&lt;/li&gt;

&lt;li&gt;Cloud or EC2 integration or support&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;other_considerations&quot;&gt;Other Considerations&lt;/h2&gt;

&lt;p&gt;There are also licensing, product maturity, and other factors worth considering.&lt;/p&gt;

&lt;p&gt;Below are some additional factors:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Licensing Agreement Factors
&lt;ul&gt;
&lt;li&gt;Product Maturity and Stability&lt;/li&gt;

&lt;li&gt;Growth Potential and Velocity of the Product&lt;/li&gt;

&lt;li&gt;The product’s FOSS &amp;amp; SW Dependencies required&lt;/li&gt;

&lt;li&gt;How easy it is to Install and is there existing DevOps Automation?&lt;/li&gt;

&lt;li&gt;Security&lt;/li&gt;

&lt;li&gt;API Exposure and cleanliness of Architecture&lt;/li&gt;

&lt;li&gt;Product’s complexity and learning curve&lt;/li&gt;

&lt;li&gt;Software Language support and requirements&lt;/li&gt;

&lt;li&gt;Ease of integrating and how it compliments your existing environment&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;survey&quot;&gt;Survey&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/survey1.jpg&quot; alt=&quot;Streaming Survey&quot; /&gt; &lt;em&gt;Big Data Streaming Engines&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;These products represent some of the core current players. Of the current products Storm currently has the widest adoption and is the most mature.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/survey2.jpg&quot; alt=&quot;Streaming Survey (continued)&quot; /&gt; &lt;em&gt;Additional Relevant Engines&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;These are some additional products worth considering. Some of these products are more focused on specialized requirements. &lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;recommendations&quot;&gt;Recommendations&lt;/h2&gt;

&lt;p&gt;Although still technically Beta, &lt;a href=&quot;http://storm-project.net/&quot;&gt;Storm&lt;/a&gt; is currently the most mature product available for general Stream processing. The product is currently in use and proven to scale at a large number of companies and government agencies. The product has also recently been adopted for Incubation under the Apache Foundation, which is another good sign for the product’s longevity. While the architecture is a bit complex and there is some learning curve involved (especially for more complex processing and how it does book keeping), it is pretty simple to get started with Storm and the product has the versatility to grow with your organization and use case.&lt;/p&gt;

&lt;p&gt;However, if your processing flows are less complex (read, perform some operations, then write) and you operate in &lt;a href=&quot;http://aws.amazon.com/&quot;&gt;AWS&lt;/a&gt; or use any of the Amazon AWS services, &lt;a href=&quot;http://aws.amazon.com/kinesis/&quot;&gt;Kinesis&lt;/a&gt; while still beta might be a very good alternative to consider due to its simplicity, ease of integration, and management within AWS.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://samza.incubator.apache.org/&quot;&gt;Samza&lt;/a&gt; out of LinkedIn (the same Organization who made &lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; which many of these streaming products require) represents another promising alternative. Architectully it is less complex than Storm and it includes some interesting features out of the box. It’s use of YARN as a Resource manage (out of the next generation Hadoop ecosystem) will also likely be a significant strength for the product. However, Samza (also an Apache Incubator) is still beta without an official release version so it represents greater risk in its adoption.&lt;/p&gt;

&lt;p&gt;If your problem is more specialized you might also want to consider some of the more specialized products listed below in this survey. For example look at &lt;a href=&quot;http://www.hazelcast.com/&quot;&gt;Hazelcast&lt;/a&gt; if your needs are more suited toward an In-Memory-Datagrid or there are also several products more tightly focused on Complex Event Processing (CEP) which might warrant consideration.&lt;/p&gt;

&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;p&gt;Project homepages for those looked at in this survey.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://storm-project.net/&quot;&gt;Storm&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href=&quot;http://aws.amazon.com/kinesis/&quot;&gt;Kinesis&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href=&quot;http://samza.incubator.apache.org/&quot;&gt;Samza&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href=&quot;http://projects.spring.io/spring-xd/&quot;&gt;Spring XD&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href=&quot;http://incubator.apache.org/s4/&quot;&gt;S4&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href=&quot;http://www.hazelcast.com/index.jsp&quot;&gt;Hazelcast&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href=&quot;http://wso2.com/products/complex-event-processor/&quot;&gt;WS02&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href=&quot;http://blog.clojurewerkz.org/blog/2013/08/29/stream-processing-with-eep/&quot;&gt;EEP&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href=&quot;https://github.com/walmartlabs/mupd8&quot;&gt;MUPD8&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href=&quot;https://github.com/Dempsy/Dempsy&quot;&gt;Dempsy&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href=&quot;http://spark.incubator.apache.org/docs/latest/streaming-programming-guide.html&quot;&gt;Spark Streaming&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
				<pubDate>Fri, 13 Dec 2013 18:00:04 +0000</pubDate>
				<link>http://owaaa.github.io/bigdata/streaming/ec2/amazon/architecture/2013/12/13/mongo_cloudwatch.html</link>
				<guid isPermaLink="true">http://owaaa.github.io/bigdata/streaming/ec2/amazon/architecture/2013/12/13/mongo_cloudwatch.html</guid>
			</item>
		
	</channel>
</rss>
